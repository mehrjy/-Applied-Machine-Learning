\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{chngcntr}
\usepackage{titlesec}
\usepackage{dsfont}
\usepackage{pifont}



%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}




\begin{document}

\begin{center}
{\Large Machine learning \hspace{0.5cm} HW 4}\\
\textbf{\small{Understanding Machine Learning: From Theory to Algorithms}[Exercises for chapters 10,11 and 18]}\\
\textbf{
Mehrnaz jalili:400422061}
\textbf{
Arash Sajjadi:400422096}\\ %You should put your name here
\textit{Email: \href{mailto:Mehrnazjalili1991@gmail.com}{Mehrnazjalili1991@gmail.com}} 

\textit{Email: \href{mailto:arash.sajjadi@yahoo.com}{arash.sajjadi@yahoo.com}} 


Date: \today %You should write the date here.
\end{center}

\vspace{0.2 cm}

\section*{chapter 10}
\subsection*{Exercise 10.1:}

$\epsilon, \delta\in\left ( 0,1 \right )$  Pick $k$ “chunks” of size $m_\mathcal{H}\left ( \epsilon/2 \right )$. Apply A on each of these chunks, to obtain $\hat{h}_1,\hdots,\hat{h}_k$ \footnote{ Note that the probability that $\min_{i\in [k]}L_\mathcal{D}(h)\leq \min L_\mathcal{D}(h)+\frac{\epsilon}{2}$ is at least $1-\delta_0^k\geq 1-\frac{\delta}{2}$}  Now,
apply an ERM over $\hat{\mathcal{H}}$. Note that $\hat{\mathcal{H}}:=\left \{ \hat{h}_1,\hdots,\hat{h}_k \right \}$  with the training data being the last chunk of size $\left \lceil \frac{2}{\epsilon^2} .\log\left ( \frac{4k}{\delta} \right ) \right \rceil$  Denote the output hypothesis by $\hat{h}$.We also should claim that with probability at least $1-\delta/2$, $L_\mathcal{D}(\hat{h})\leq \min_{i\in [k]} L_\mathcal{D}(h_i)+\frac{\epsilon}{2}$. Now we have: $L_\mathcal{D}(\hat{h})\leq \min_{i\in [k]} L_\mathcal{D}(h)+\epsilon$



%\subsection*{Exercise 10.4:}




\section*{chapter 11}
\subsection*{Exercise 11.1:}

Consider a case in that the label is chosen at random according to $\mathbb{P}\left [ y = 1 \right ]= \left [ y=0 \right ]=\frac{1}{2}$ Consider a learning algorithm that outputs the constant predictor $h(x)=1$ if the parity of the labels on the training set is 1 and otherwise the algorithm outputs the
constant predictor $h(x)=0$. Prove that the difference between the leave-one-out estimate and the true error in such a case is always $\frac{1}{2}$.


first consider S set as a i.i.d sample


we know h as the out put of learning algorithm.


because h is a constant function we have $L_{D}(h)=\frac{1}{2}$
we want to calculate the $L_{V}(h)$. assume the parity of S is 1


then fix some fold $\left\{ (x,y)\right\}\subseteq S$


we have two cases bellow:


\begin{itemize}
    \item as the $S \setminus \left\{ X\right\}$ is 1 so $Y=0$ and since trained using $S \setminus \left\{ X\right\}$ the algorithm oytputs the predictor $h(x)=1$\\
therefor the leave-one-out estimate using this fold is 1.\\
    \item as the $S \setminus \left\{ X\right\}$ is 0 so $Y=1$ and since trained using $S \setminus \left\{ X\right\}$ the algorithm outputs the predictor $h(x)=0$\\
Therefore the leave-one-out estimate using this fold is 1.\\
 after averaging the two folds, we calculate, we find out that the estimated error of h is 1. and the difference between estimation error and the true error is $\frac{1}{2}$.\\
 for the other case (the parity be 0) analyze in the same way.

\end{itemize}

\subsection*{Exercise 11.2:}
consider $\mathcal{H}_1 \subseteq \mathcal{H}_2 \subseteq \hdots \subseteq \mathcal{H}_k$ and also, $\forall i \in k, \hspace{0.3cm} |\mathcal{H}_i|=2^i$.Learning $\mathcal{H}_k$ in the Agnostic-Pac model provides the following bound for an ERM hypothesis $h$:
\begin{equation*}
    L_{\mathcal{D}}(h)\leq \min_{h\in \mathcal{H}_k}L_{\mathcal{D}}(h)+\left ( \frac{2}{m}.(k+1+\log(\frac{1}{\delta}) \right )^{\frac{1}{2}}
\end{equation*}
Alternatively, we can use model selection as we describe next. let us to assume that $j$ is the minimal index which contains a hypothesis $h^* \in \text{argmin}_{h\in \mathcal{H}}L_{\mathcal{D}}(h)$. In tgis stage we try to fix some $r \in [k]$. Now according to Hoeffding’s inequality, with probability at least $\frac{1-\delta}{2k}$, we have:
\begin{equation*}
    |L_\mathcal{D}(\hat{h}_r)-L_V(\hat{h}_r)|\leq \left ( \frac{\log (4/ {\delta})}{2\alpha m} \right )^{\frac{1}{2}}
\end{equation*}
by applying the union bound we can claim that, In particular, with probability at least  $1-\delta$ we have $L_\mathcal{D}(\hat{h})\leq L_\mathcal{D}(\hat{h}_j)+\sqrt{\frac{2.\log(4k/\delta)}{\alpha m}}$
Using similar arguments, we obtain that with probability at least
$1-\delta/2$,
\begin{equation*}
    L_\mathcal{D}(\hat{h}_j)\leq L_\mathcal{D}(h^*)+\sqrt{\frac{2\log (4|\mathcal{H}_j|/\delta)}{m-m\alpha}}
\end{equation*}

Combining the two last inequalities with the union bound, we obtain
that with probability at least $1-\delta$:
\begin{equation*}
    L_\mathcal{D}(\hat{h})\leq L_\mathcal{D}(h^*)+\sqrt{\frac{2\log(4k/\delta)}{\alpha m}}+\sqrt{\frac{2(j+\log(4/\delta))}{(1-\alpha)m}}
\end{equation*}

Comparing the two bounds, we see that when the “optimal index” $j$ is significantly smaller than k, the bound achieved using model selection is much better. Being even more concrete, if $j$ is logarithmic in $k$, we achieve a logarithmic improvement.

\section*{chapter 18}
\subsection*{Exercise 18.2:}

















in first iteration we compute the information gain:
\begin{align*}
    H(Y)=-\frac{1}{2} \log(\frac{1}{2})- \frac{1}{2} \log(\frac{1}{2})=1
\end{align*}
\begin{gather*}
    IG(X_{1})=H(Y)-H(Y|X_{1}) \\
    = 1-\left [ \left ( \frac{3}{4} \right ) \left ( \left ( -\frac{2}{3} \right )\log\left ( \frac{2}{3} \right ) \right )- \frac{1}{3} \log\frac{1}{3} + \frac{1}{4}\left ( -0\log0 -1\log1 \right )\right ] \\= 1-\left ( \frac{3}{4} \right )\left [ -\frac{2}{3}\log\left ( \frac{2}{3} \right )-\frac{1}{3}\log\left ( \frac{1}{3} \right ) \right ]> 0
\end{gather*}
\begin{gather*}
    IG(X_{2})=H(Y)-H(Y|X_{2})\\
    = 1-\left [ \left ( \frac{1}{2} \right ) \left ( \left ( -\frac{1}{2} \right )\log\left ( \frac{1}{2} \right ) \right )- \frac{1}{2} \log\frac{1}{2} + \frac{1}{2}\left ( -\frac{1}{2}\log\frac{1}{2} -\frac{1}{2}\log\frac{1}{2} \right )\right ] \\
    = 1-\left[\frac{1}{2}(-1) + \frac{1}{2}(-1)\right ]=0
\end{gather*}

\begin{gather*}
    IG(X_{3})=H(Y)-H(Y|X_{3})\\
    = 1-\left [ \left ( \frac{1}{2} \right ) \left ( \left ( -\frac{1}{2} \right )\log\left ( \frac{1}{2} \right ) \right )- \frac{1}{2} \log\frac{1}{2} + \frac{1}{2}\left ( -\frac{1}{2}\log\frac{1}{2} -\frac{1}{2}\log\frac{1}{2} \right )\right ] \\
    = 1-\left[\frac{1}{2}(-1) + \frac{1}{2}(-1)\right ]=0
\end{gather*}

so we choose $X_{1}=0$ for begin the tree.
1. for choosing the left node:
\begin{center}
    $ID3\left ( \left\{ \left ( \left ( 1,1,1 \right ) ,1\right ), \left ( \left ( 1,0,0 \right ),1 \right ), \left ( \left ( 1,1,0 \right ),0 \right )\right\},\left\{ x_{2}, x_{3}\right\} \right )$
\end{center}
we have to compute the info. gain again
\begin{align*}
    H(Y)=-\frac{2}{3} \log(\frac{2}{3})- \frac{1}{3} \log(\frac{1}{3})
\end{align*}

\begin{gather*}
    IG(X_{2})=H(Y)-H(Y|X_{2})\\
    =H(Y)- \left [\frac{2}{3} \left ( -\frac{1}{2} \log\frac{1}{2} \right ) -\frac{1}{2} \log\frac{1}{2} \right ) \right ]\\
    = H(Y)- \frac{2}{3}
\end{gather*}

\begin{gather*}
    IG(X_{3})=H(Y)-H(Y|X_{3})\\
    =H(Y)- \left [\frac{2}{3} \left ( -\frac{1}{2} \log\frac{1}{2} \right ) -\frac{1}{2} \log\frac{1}{2} \right ) \right ]\\
    = H(Y)- \frac{2}{3}
\end{gather*}
its possible to choose either $X_{2}$ or $X_{3}$ to have 2 different trees.

2. right node:
\begin{center}
    $ID3\left ( \left\{\left ( \left ( 0,0,1 \right ),0 \right ) \right\},\left\{x_{2},x_{3} \right\} \right )$
\end{center}
the only possible label is 0. training error for FIRST tree is $\frac{1}{4}$ because the only mislabeled point is $\left (  \left ( 1,1,1 \right ),1 \right )$\\



\begin{center}
    

\usetikzlibrary{positioning}
\newdimen
\nodeDist
\nodeDist=20mm
\begin{tikzpicture}[
    node/.style={%
      draw,
      rectangle,
    },
  ]

    \node [node] (A) {$x_{1}=0?$};
    \path (A) ++(-135:\nodeDist) node [node] (B) {$x_{2}=0?$};
    \path (B) ++(-135:\nodeDist) node [node] (D) {0};
    \path (B) ++(-45:\nodeDist) node [node] (E) {1};
    \path (A) ++(-45:\nodeDist) node [node] (C) {0};
    

    \draw (A) -- (B) node [left,pos=0.25] {F}(A);
    \draw (A) -- (C) node [right,pos=0.25] {T}(A);
    \draw (B) -- (D) node [left,pos=0.25] {F}(A);
    \draw (B) -- (E) node [right,pos=0.25] {T}(A);
\end{tikzpicture}\\



\vspace{1cm}






\begin{tikzpicture}[
    node/.style={%
      draw,
      rectangle,
    },
  ]

    \node [node] (A) {$x_{1}=0?$};
    \path (A) ++(-135:\nodeDist) node [node] (B) {$x_{3}=0?$};
    \path (B) ++(-135:\nodeDist) node [node] (D) {0};
    \path (B) ++(-45:\nodeDist) node [node] (E) {1};
    \path (A) ++(-45:\nodeDist) node [node] (C) {0};
    

    \draw (A) -- (B) node [left,pos=0.25] {F}(A);
    \draw (A) -- (C) node [right,pos=0.25] {T}(A);
    \draw (B) -- (D) node [left,pos=0.25] {F}(A);
    \draw (B) -- (E) node [right,pos=0.25] {T}(A);
\end{tikzpicture}\\


%\end{center}

and the training error for the second tree is also $\frac{1}{4}$ because the only mislabeled point is $((1,0,0),1)$\\
so the training error for any tree with the 2 depth with ID3 is at least $\frac{1}{4}$\\

We want to show the decision tree with the 0 training error.
%\begin{center}
    

\begin{tikzpicture}[
    node/.style={%
      draw,
      rectangle,
    },
  ]
    \node [node] (A) {$x_{2}=0?$};
    \path (A) ++(-135:\nodeDist) node [node] (B) {$x_{3}=0?$};
    \path (B) ++(-135:\nodeDist) node [node] (D) {1};
    \path (B) ++(-55:\nodeDist) node [node] (E) {0};
    \path (A) ++(-45:\nodeDist) node [node] (C) {$x_{3}=0?$};
    \path (C) ++(-125:\nodeDist) node [node] (F) {0};
    \path (C) ++(-45:\nodeDist) node [node] (G) {1};
    \draw (A) -- (B) node [left,pos=0.25] {F}(A);
    \draw (A) -- (C) node [right,pos=0.25] {T}(A);
    \draw (B) -- (D) node [left,pos=0.25] {F}(A);
    \draw (B) -- (E) node [right,pos=0.25] {T}(A);
    \draw (C) -- (F) node [left,pos=0.25] {F}(A);
    \draw (C) -- (G) node [right,pos=0.25] {T}(A);
\end{tikzpicture}\\

\end{center}









\end{document}


